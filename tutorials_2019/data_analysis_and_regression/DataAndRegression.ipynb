{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "1. Python and Jupyter Intro\n",
    "1. Dataset examination\n",
    "1. Linear Regression and Gradient Descent\n",
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many thanks and indebted to Daniel Bowring, Chris Mayes, Auralee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Learn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python and Jupyter Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "# Numpy: for scientific computing and arrays\n",
    "import numpy as np\n",
    "# Matplotlib: for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Scipy: For maths, science, engineering\n",
    "import scipy.stats as stats\n",
    "# Seaborn: high level plots\n",
    "import seaborn as sns\n",
    "# Pandas: data analysis package\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Array\n",
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect value \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python starts counting at 0!\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy array manipulation\n",
    "a * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Array\n",
    "b = np.array([[1,2],[4,5],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often useful to get shape of matrix\n",
    "np.shape(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b[1] is short for\n",
    "b[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access column\n",
    "b[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary is a collection which is unordered, changeable and indexed. In Python dictionaries are written with curly brackets, and they have keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisWorkshop = {\n",
    "  \"name\": \"2nd ICFA Mini-Workshop on Machine Learning for Charged Particle Accelerators\",\n",
    "  \"place\": \"PSI\",\n",
    "  \"year\": 2019\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisWorkshop['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, np.sqrt(a), 'o-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beamline with two solenoids (strengths $K_1$, $K_2$), two cavities (strengths $G_1$, $G_2$ phases $\\phi_1$, $\\phi_2$).\n",
    "\n",
    "Simulated with beam dynamics code OPAL (https://gitlab.psi.ch/OPAL/Manual-2.0/wikis/home)\n",
    "\n",
    "Outputs: emittances, energy, beamsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many thanks to Auralee, Nicole Neveu and Andreas Adelmann for providing the dataset!\n",
    "\n",
    "Based on a model of the Argonne Wakefield Accelerator.\n",
    "\n",
    "Note that the dataset is not the same as a soon to be published paper on accelerator surrogate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Beamline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filename = \"ANN_7ksliced.pk\"\n",
    "data = np.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0: inspect data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does our data format look like?\n",
    "# Your code here, see notebook for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format not so useful for data analysis so we will transform to something more suitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays\n",
    "dname  = np.array(data[0]['dvarNames']) # input (design) names\n",
    "oname  = np.array(data[0]['objNames'])  # output names\n",
    "names  = data[0]['dvarNames'] + data[0]['objNames'] # all names\n",
    "dval   = np.asarray(data[1]['dvarValues'],dtype=np.float64) # convert string to float\n",
    "oval   = np.asarray(data[1]['objValues'],dtype=np.float64)  # output values\n",
    "values = np.concatenate((dval,oval),axis=1) # all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas \"DataFrame\" : \"Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data    = values,\n",
    "                  columns = names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the data look like?\n",
    "# Do we need to clean the data?\n",
    "# Which parameter can we throw away?\n",
    "# Your code here, see notebook for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be no clear outliers.\n",
    "Output variable 'numParticles' not interesting \n",
    "\n",
    "For the rest no cleaning needed (unusual!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throw away 'numParticles' column (hackish way)\n",
    "dname  = np.array(data[0]['dvarNames']) # input (design) names\n",
    "oname  = np.array(data[0]['objNames'][:-1])  # output names\n",
    "names  = data[0]['dvarNames'] + data[0]['objNames'][:-1] # all names\n",
    "dval   = np.asarray(data[1]['dvarValues'],dtype=np.float64) # convert string to float\n",
    "oval   = np.asarray(data[1]['objValues'],dtype=np.float64)[:,:-1]  # output values\n",
    "values = np.concatenate((dval,oval),axis=1) # all values\n",
    "df     = pd.DataFrame(data    = values,\n",
    "                      columns = names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some patterns seen, let's try to find some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Plot\n",
    "correlations = np.empty((len(dname),len(oname)))\n",
    "for i in range(len(dname)):\n",
    "    for j in range(len(oname)):\n",
    "            #compute correlation\n",
    "            correlations[i,j],_ = stats.spearmanr(dval[:,i], oval[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlations\n",
    "fig = plt.figure(figsize=((8,6))) # slightly larger\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations,cmap='seismic')\n",
    "fig.colorbar(cax)\n",
    "plt.xticks(range(len(oname)), oname)\n",
    "plt.yticks(range(len(dname)), dname);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the energy correlation\n",
    "G1     = dval[:,3]\n",
    "energy = oval[:,7]\n",
    "plt.plot(G1,energy,'.');\n",
    "plt.xlabel(dname[3])\n",
    "plt.ylabel(oname[7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict energy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LinReg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind linear regression:\n",
    "\n",
    "$\\hat{y}=\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n = \\theta^T\\cdot\\mathbf{x} = h_\\theta(x)$\n",
    "\n",
    "Mean square error: our cost function\n",
    "\n",
    "$\\mathrm{MSE}(X,h_\\theta)=\\frac{1}{m}\\sum^{m}_{i=1}(\\theta^T\\cdot x^{(i)}-y^{(i)})^2$\n",
    "\n",
    "The normal equation is an analytic solution that minimizes $\\theta$:\n",
    "\n",
    "$\\hat{\\theta}=(X^T\\cdot X)^{-1}\\cdot X^T\\cdot y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best-fit according to mean square error has an analytic solution: the normal equation.\n",
    "# RMS error also ok, but more computationally involved.\n",
    "G1_b = np.c_[np.ones((len(G1),1)),G1] # 1's and G1\n",
    "theta_best = np.linalg.inv(G1_b.T.dot(G1_b)).dot(G1_b.T).dot(energy)\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G1,energy,'.');\n",
    "plt.xlabel(dname[3])\n",
    "plt.ylabel(oname[7]);\n",
    "plt.plot(G1, theta_best[0]+theta_best[1]*G1, 'r', linewidth=2, label='mean square fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on our model\n",
    "error1 = energy - (theta_best[0]+theta_best[1]*G1)\n",
    "rel_error1 = error1 / energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rel_error1);\n",
    "plt.xlabel(\"relative error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Add second parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, see notebook for an example\n",
    "# What is the error on the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do all this again, using sklearn. Will be easier to code this time.\n",
    "# scikit-learn: Machine Learning in Python\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(dval, energy) # Let's use all input information this time\n",
    "\n",
    "# The fit method imparts attributes intercept_ and coef_, which are just what you think they are.\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction from our model\n",
    "prediction = lin_reg.predict(dval)\n",
    "rel_error_all = (prediction - energy) / energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rel_error1, label='1 parameter');\n",
    "plt.hist(rel_error_all, label = 'all parameters');\n",
    "plt.xlabel(\"relative error\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated functions, there is no analytical solution for the best fit.\n",
    "Now we'll investigate the 'machine learning' way of finding the best fit.\n",
    "\n",
    "Define a cost fuction. At feature coordinates, calculate cost function gradient. \"Go down.\"\n",
    "\n",
    "Good policy to make sure all features have similar scale. Small partial derivatives will make search times longer.\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_j}\\mathrm{MSE}(\\theta)=\\frac{2}{m}\\sum_{i=1}^m(\\theta^T\\cdot x^{(i)}-y^{(i)})x^{(i)}_j$\n",
    "\n",
    "$\\theta_{i+1} = \\theta_i-\\eta\\nabla_\\theta\\mathrm{MSE}(\\theta)$\n",
    "\n",
    "$\\eta=$ training rate, determining size of step in min search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd(X, y, eta, n_iteration, theta0):\n",
    "    # batch gradient descent: use whole training set to compute gradient at every iteration\n",
    "    # X = observed data / \"features\"\n",
    "    # y = target \n",
    "    # eta = training rate\n",
    "    # n_iteration = define total number of steps in search\n",
    "    # theta0 = starting guess\n",
    "    batch_thetas = np.empty((n_iteration,2)) # vector of fit parameters found by *batch* gradient descent (see below)\n",
    "    X_b   = np.c_[np.ones((len(y),1)),X] # add 1 vector for offset\n",
    "    y     = y.reshape(-1,1)              # reshape if not done\n",
    "    m     = len(y)                       # vector length\n",
    "    theta = theta0\n",
    "    \n",
    "    for iteration in range(n_iteration):\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "        theta = theta - eta*gradients\n",
    "        batch_thetas[iteration] = theta.T # populate batch_thetas vector with search steps\n",
    "    return theta, batch_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation array: data between [0,1]\n",
    "def norm(x):\n",
    "    return (x-min(x))/(max(x)-min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise our parameters (you can also try to see what happens without)\n",
    "G1_norm = norm(G1) # Normalise\n",
    "E_norm  = norm(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new solution for normalised values\n",
    "lin_reg.fit(G1_norm.reshape(-1,1), E_norm)\n",
    "# The fit method imparts attributes intercept_ and coef_, which are just what you think they are.\n",
    "theta0_best = np.array([lin_reg.intercept_, lin_reg.coef_[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# How long does this gradient search take?\n",
    "\n",
    "# How does training rate affect fit quality, for a constant n_iteration?\n",
    "# Make a vector of etas.\n",
    "etas = [0.1, 0.01, 0.001]\n",
    "n_iteration = 1000\n",
    "batch_thetas = np.empty((len(etas), n_iteration, 2))\n",
    "thetas       = np.empty((len(etas), 2))\n",
    "for i, eta in enumerate(etas):            # enumerate for loop: both index and value \n",
    "    theta0 = np.random.randn(2,1)         # pick a random starting point in feature-space\n",
    "    # Fit results from len(etas) different gradient descent searches\n",
    "    (thetas[i,0], thetas[i,1]), batch_thetas[i] = bgd(G1_norm, E_norm, eta, n_iteration, theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(G1_norm, E_norm) # same as default plot\n",
    "colors = ['orange','cyan','magenta']\n",
    "for i, eta in enumerate(etas):\n",
    "    # Plot each search result\n",
    "    ax1.plot(G1_norm, thetas[i,0]+G1_norm*thetas[i,1], label='$\\eta=$'+str(eta), color=colors[i])\n",
    "ax1.plot(G1_norm, theta0_best[0]+theta0_best[1]*G1_norm, 'r:', linewidth=2, label='normal equation')\n",
    "plt.legend()\n",
    "plt.xlabel(dname[3])\n",
    "plt.ylabel(oname[7]);\n",
    "plt.title('Different training rates');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch gradient descent: plot coefficients in theta-space for each step in BGD.\n",
    "for i, eta in enumerate(etas):\n",
    "    plt.plot(batch_thetas[i][:,0], batch_thetas[i][:,1], '.-', label = '$\\eta=$'+str(eta))\n",
    "plt.plot([theta0_best[0]], [theta0_best[1]], 'k*', label='best fit')\n",
    "plt.xlabel(\"theta_0\")\n",
    "plt.ylabel(\"theta_1\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise interactively\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_xlabel('theta0')\n",
    "ax2.set_ylabel('theta1')\n",
    "ax2.scatter(G1_norm, E_norm)\n",
    "ax2.plot(G1_norm, theta0_best[0]+theta0_best[1]*G1_norm, 'r-.', linewidth=2, label='normal equation')\n",
    "ax2.legend()\n",
    "for th in batch_thetas[1][::100]: # every 100 iterations\n",
    "    # watch the fit converge as we get closer to cost minimum\n",
    "    ax2.plot(G1_norm, th[0]+G1_norm*th[1])\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.1)\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent\n",
    "-------\n",
    "\n",
    "At every step, pick a random instance of training set and only compute gradient on that instance\n",
    "\n",
    "This is a random walk. Cost function won't relate monotonically to iteration step number.\n",
    "\n",
    "Con = noise, even at cost minimum\n",
    "\n",
    "Pro = more resistant to local fake-me-out minima\n",
    "\n",
    "Compromise also possible: Large steps at first to jump out of local minima, and smaller steps once you start converging on the true minimum. *Simulated annealing*, based on a *learning schedule* which tunes $\\eta$ per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of those field-specific conventions.\n",
    "# Iterations clumped into m groups: \"epochs\".\n",
    "n_epochs = 20\n",
    "\n",
    "# learning schedule hyperparameters\n",
    "# set up a learning schedule so eta varies by iteration step\n",
    "t0, t1 = 5, 20\n",
    "def learning_schedule(t):\n",
    "    # Learning schedule \n",
    "    return t0 / (t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, n_epochs, theta0):\n",
    "    sgd_thetas = np.empty((n_epochs*len(y),2)) # vector of fit parameters found by *batch* gradient descent (see below)\n",
    "    X_b   = np.c_[np.ones((len(y),1)),X] # add 1 vector for offset\n",
    "    y     = y.reshape(-1,1)              # reshape if not done\n",
    "    m     = len(y)                       # vector length\n",
    "    theta = theta0\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X_b[random_index:random_index+1]\n",
    "            yi = y  [random_index:random_index+1]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            eta = learning_schedule(epoch*m + i)\n",
    "            theta = theta - eta*gradients    \n",
    "            sgd_thetas[i + epoch*m] = theta.T\n",
    "    return theta, sgd_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta0 = np.random.randn(2,1)         # pick a random starting point in feature-space\n",
    "theta, sgd_thetas = sgd(G1_norm, E_norm, n_epochs, theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)\n",
    "print('Difference relative to normal equation:', theta[0]-theta0_best[0], theta[1]-theta0_best[1])\n",
    "# Compare: 20 steps for stochastic gradient descent, relative to 1000 for batch GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch gradient descent\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compromise between computing the true gradient and the gradient at a single example is to compute the gradient against more than one training example (called a \"mini-batch\") at each step.\n",
    "\n",
    "This can perform significantly better than \"true\" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately. It may also result in smoother convergence, as the gradient computed at each step is averaged over more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbgd(X, y, n_epochs, mb, theta0):\n",
    "    # mb  minibatch size\n",
    "    m = len(y)               # vector length\n",
    "    n_mb = m // mb           # number of minibatches (// integer division) \n",
    "    sgd_thetas = np.empty((n_epochs*n_mb,2)) # vector of fit parameters found by *batch* gradient descent (see below)\n",
    "    X_b   = np.c_[np.ones((len(y),1)),X] # add 1 vector for offset\n",
    "    y     = y.reshape(-1,1)              # reshape if not done\n",
    "    theta = theta0\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(n_mb):\n",
    "            random_index = np.random.randint(m - mb)\n",
    "            xi = X_b[random_index:random_index+mb]\n",
    "            yi = y  [random_index:random_index+mb]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)/mb\n",
    "            eta = learning_schedule(epoch*m + i)\n",
    "            theta = theta - eta*gradients    \n",
    "            sgd_thetas[i + epoch * n_mb] = theta.T\n",
    "    return theta, sgd_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mini_batch_size=30\n",
    "n_epochs = 50\n",
    "theta0 = np.random.randn(2,1)         # pick a random starting point in feature-space\n",
    "theta, mb_thetas = mbgd(G1_norm, E_norm, n_epochs, mini_batch_size, theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at progress in theta-space for each technique.\n",
    "fig4, ax4 = plt.subplots(figsize=(15,9))\n",
    "bd_thetas = batch_thetas[0] # 0: best learning rate of 0.01\n",
    "ax4.plot(sgd_thetas[:,0],sgd_thetas[:,1],   '.-', label='stochastic gradient descent')\n",
    "ax4.plot(mb_thetas[:,0], mb_thetas[:,1],    '.-', label='minibatch gradient descent')\n",
    "ax4.plot(bd_thetas[:,0], bd_thetas[:,1], '.-', label='batch gradient descent')\n",
    "ax4.set_xlabel('theta_0')\n",
    "ax4.set_ylabel('theta_1')\n",
    "ax4.plot(theta0_best[0], theta0_best[1], 'ro', label='true optimum')\n",
    "\n",
    "ax4.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curious about error per iteration for each technique.\n",
    "def getRMS(x, y, x0, y0):\n",
    "    return ((x-x0)**2+(y-y0)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_rms = getRMS(sgd_thetas[:,0],sgd_thetas[:,1],theta0_best[0],theta0_best[1])\n",
    "mb_rms  = getRMS(mb_thetas [:,0],mb_thetas [:,1],theta0_best[0],theta0_best[1])\n",
    "bd_rms  = getRMS(bd_thetas [:,0],bd_thetas [:,1],theta0_best[0],theta0_best[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rms error vs number of steps\n",
    "plt.figure(figsize=((10,5)))\n",
    "plt.subplot(121)\n",
    "plt.semilogx(bd_rms,  label='batch')\n",
    "plt.semilogx(sgd_rms, label='stochastic')\n",
    "plt.semilogx(mb_rms,  label='minibatch')\n",
    "plt.subplot(122)\n",
    "plt.loglog(bd_rms,  label='batch')\n",
    "plt.loglog(sgd_rms, label='stochastic')\n",
    "plt.loglog(mb_rms,  label='minibatch')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "\n",
    "Now do it with sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/sgd.html#regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor # more info with ?SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd_reg = SGDRegressor(max_iter=100, penalty=None, eta0=0.1, tol=1e-3)\n",
    "# default learning schedule\n",
    "# no regularization --> penalty=None\n",
    "# tol: tolerance of final solution\n",
    "# Faster so no need to slice!\n",
    "G1_norm = norm(G1) # Normalise\n",
    "E_norm  = norm(energy)\n",
    "G1_norm = G1_norm.reshape(-1, 1) # reshape 1-d to 2-d array\n",
    "sgd_reg.fit(G1_norm,E_norm); # reshape into 2-d array, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Difference relative to normal equation:', sgd_reg.intercept_ - theta0_best[0], sgd_reg.coef_ - theta0_best[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_norm = sgd_reg.predict(G1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions_norm - E_norm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalise\n",
    "def unnorm(x, minx, maxx):\n",
    "    return (x*(maxx-minx) + minx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = unnorm(predictions_norm, min(energy), max(energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error = (predict-energy)/energy\n",
    "plt.hist(rel_error)\n",
    "plt.xlabel(\"relative error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Predict $\\epsilon_x$ using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, see notebook for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(x)=\\frac{1}{1+e^{-k(x-x_0)}}$\n",
    "\n",
    "$x_0$ can be thought of as a bias: how positive/negative does x have to be before the neuron fires?\n",
    "\n",
    "Action of neural network looks something like this:\n",
    "\n",
    "$a^{(1)}=\\sigma(\\mathbf{W}a^{(0)}+b)$\n",
    "\n",
    "The logistic function can be used as a binary classifier. Computes a weighted sum of input features, but the output is the logistic of that sum. \n",
    "\n",
    "$\\hat{p}=\\sigma(\\theta^T\\cdot x)$\n",
    "\n",
    "Model estimates the probability $\\hat{p}$ that an instance $x$ belongs to \"positive class\". This is the basis for predictions under this method.\n",
    "\n",
    "$\\hat{y} =\\left\\{\\begin{array}{c}0,\\;\\hat{p}<0.5\\\\1,\\;\\hat{p}\\ge0.5\\end{array}\\right.$\n",
    "\n",
    "Training based on a cost function kinda like\n",
    "\n",
    "$c(\\theta)=\\left\\{\\begin{array}{c}-\\log(\\hat{p}),\\quad y=1\\\\ -\\log(1-\\hat{p}),\\quad y=0\\end{array}\\right.$\n",
    "\n",
    "with high cost for probabilities close to zero. No analytic solution to the minimum of this cost function, but the function is convex so it's reasonable to try GD on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a change we use pandas dataframe\n",
    "df.hist('emit_x')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0));\n",
    "df.hist('emit_y')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define good machines as having small final emittance\n",
    "goodMachines = [(df['emit_x'] < 4e-6)] and [(df['emit_y'] < 4e-6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of the good machines\n",
    "np.count_nonzero(goodMachines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['K2'], goodMachines);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict good machines based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = np.array(goodMachines).ravel() # ravel to make 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K2 = df['K2'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression() # more info with ?LogisticRegression\n",
    "log_reg.fit(K2,classifier) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg.predict(K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy (how many are correctly predicted?)\n",
    "log_reg.score(K2, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = log_reg.decision_function(K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidence_scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More metrics\n",
    "from sklearn.metrics import recall_score     # true positives / (true pos. + false neg.)\n",
    "from sklearn.metrics import precision_score  # true positives / (true pos. + false pos.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many good machines are well predicted?\n",
    "recall_score(classifier, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many predicted machines are actually good?\n",
    "precision_score(classifier, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve (Receiver Operating Characteristic)\n",
    "# False positive rate versus true positive rate for different thresholds \n",
    "# (usually 0.5) but can be chosen\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(classifier, confidence_scores, pos_label=True)\n",
    "roc_auc = auc(fpr, tpr) # area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Use all input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# How does the accuracy change with more input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![title](LearnLearn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
