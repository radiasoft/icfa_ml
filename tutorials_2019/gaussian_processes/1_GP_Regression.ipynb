{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: GP Regression\n",
    "\n",
    "In the first part of the tutorial, we are going to use the GPy library for Gaussian process regression (https://github.com/SheffieldML/GPy). GPy is available via `pip install GPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple benchmark problem, we use a small mockup machine interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accelerator:\n",
    "    def __init__(self):\n",
    "        self.x = np.array([0.1, 0.2])\n",
    "    \n",
    "    def get(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns the signal measurement.\n",
    "        \n",
    "        Examples:\n",
    "            get()  # returns a measurement with the current parameter settings\n",
    "            get(x=[0.1,0.4])  # sets the parameters x1=0.1, x2=0.4, then returns a measurement\n",
    "            get(x1=0.2)  # sets x1=0.2 and keeps the second parameter fixed\n",
    "            get(x1=0.1, x2=0.4)  # sets x1=0.1, x2=0.4\n",
    "        \n",
    "        Optional parameters:\n",
    "            x: array-like 2d\n",
    "            x1: scalar\n",
    "            x2: scalar\n",
    "        \"\"\"\n",
    "        if 'x' in kwargs:\n",
    "            self.x = np.array(kwargs['x'])\n",
    "        else:\n",
    "            for i in range(len(self.x)):\n",
    "                self.x[i] = kwargs.get(f'x{i+1}', self.x[i])\n",
    "        \n",
    "        xx, yy = self.x[0], self.x[1]\n",
    "        y = (4. - 2.1*xx**2 + (xx**4)/3.)*(xx**2) + xx*yy + (-4. + 4*(yy**2))*(yy**2)\n",
    "        return np.maximum(-y + 2.5, 0) + np.random.normal(0,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine interface maintains a parameter vector `accelerator.x`. With a call to `accelerator.get()`, the objective signal can be measured. You can also evaluate at a new parameter using the keyword arguments `accelerator.get(x=[0.1,0.2])` or `accelerator.get(x1=0.1, x2=0.2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mockup machine interface\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# retrieving current parameter and (noisy) signal\n",
    "print(f\"The current parameter settings are: {accelerator.x}\")\n",
    "print(f\"Signal measurement: {accelerator.get():.2f}\")\n",
    "print(f\"Another measurement: {accelerator.get():.2f}\")\n",
    "\n",
    "# set x1 and x2 parameters and new measurement of signal:\n",
    "print(f\"Setting x1=0.4: New measurement: {accelerator.get(x1=0.4):.2f} at x={accelerator.x}\")\n",
    "print(f\"Setting x2=2: New measurement: {accelerator.get(x2=2):.2f} at x={accelerator.x}\")\n",
    "\n",
    "# set both parameters\n",
    "print(f\"Setting x=[0.5,0.5]: New measurement: {accelerator.get(x=[0.5,0.5]):.2f} at x={accelerator.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define some helper functions for plotting GPs. You can come back to these definitions later to understand the details of how to query the GP predictions. For now, just run the cell and go on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_gp(gp, xlim, \n",
    "               plot_data=True, \n",
    "               plot_confidence=False, \n",
    "               s=2,\n",
    "               axis=None, color='C0'):\n",
    "    \"\"\"\n",
    "    Plots a 1d gp.\n",
    "    \"\"\"\n",
    "    \n",
    "    if axis is None:\n",
    "        axis = plt.gca()\n",
    "        \n",
    "    # define an evaluation set\n",
    "    X_eval = np.linspace(*xlim, 300).reshape(-1,1)  # GPy expects shape (num_points, input_dim)\n",
    "    \n",
    "    # compute posterior mean and variance\n",
    "    Y_pred, Y_var = gp.predict_noiseless(X_eval)\n",
    "    \n",
    "    # flatten the arrays for pyplot\n",
    "    X_eval = X_eval.reshape(-1)\n",
    "    Y_pred = Y_pred.reshape(-1)\n",
    "    Y_var = Y_var.reshape(-1)\n",
    "\n",
    "    # plot mean\n",
    "    axis.plot(X_eval, Y_pred, zorder=5, color=color)\n",
    "    \n",
    "    # plot data if it's not the 'fake' data point\n",
    "    if plot_data and not (gp.X.shape[0] == 1 and gp.X[0] > 0.9*10e10):\n",
    "        axis.scatter(gp.X, gp.Y, zorder=10, marker='x', color=color)\n",
    "        \n",
    "    # plot the predictive uncertainty interval\n",
    "    if plot_confidence:\n",
    "        axis.fill_between(X_eval, Y_pred-s*np.sqrt(Y_var), Y_pred+s*np.sqrt(Y_var), alpha=0.4, color=color)\n",
    "\n",
    "    axis.set_xlim(xlim)  # set x-axis limits\n",
    "\n",
    "\n",
    "def plot_1d_gp_samples(gp, xlim, num_samples=5, plot_data=True, axis=None, color_samples='C1'):\n",
    "    \"\"\"\n",
    "    Plots `num_samples` samples of a gp.\n",
    "    \"\"\"\n",
    "    \n",
    "    if axis is None:\n",
    "        axis = plt.gca()\n",
    "        \n",
    "    X_eval = np.linspace(*xlim, 300).reshape(-1,1)  # GPy expects shape (num_points, input_dim)\n",
    "\n",
    "    # Genereate posterior samples\n",
    "    Y_sample = gp.posterior_samples_f(X_eval, size=num_samples)\n",
    "\n",
    "    # plotting\n",
    "    for i in range(Y_sample.shape[-1]):\n",
    "        axis.plot(X_eval, Y_sample[:,:,i], c=color_samples)\n",
    "    \n",
    "    # plot data if it's not the 'fake' data point\n",
    "    if plot_data and not (gp.X.shape[0] == 1 and gp.X[0] > 0.9*10e10):\n",
    "        axis.scatter(gp.X, gp.Y, marker='x', zorder=10, c='C0')\n",
    "        \n",
    "    axis.set_xlim(xlim)  # set x-axis limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a data set and fitting a GP\n",
    "\n",
    "Here we are going to evaluate our accelerator on a grid to generate a data set. We then use a GP to regress the target function.\n",
    "\n",
    "As you go on you can get back to the next cell and change the number of evaluation points to obtain a different sized data-set. Unfortunately, GPy does not allow an empty data set; to effectively get the prior distribution, we can however add a single data point far away from our domain of interest (set `n=0`).\n",
    "\n",
    "Once we have the data `X` and `Y`, we can compute the posterior GP using GPy. To do so, we first define an RBF kernel, and them get a `GPy.models.GPRegression` instance with our data and the chosen kernel. \n",
    "\n",
    "*Note that GPy always expects numpy arrays with shape (num_data_points, input_dim) for X and (num_data_points, 1) for Y as input.*\n",
    "\n",
    "#### Things to try:\n",
    "* Rerun the cell to obtain a different (noisy) measurement.\n",
    "* To change the number of evaluation points, set `n` to a different value or `n=0` to get th GP prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of evaluations\n",
    "n = 20\n",
    "\n",
    "if n > 0:\n",
    "    # evaluation points\n",
    "    X = np.linspace(-2,2,n).reshape(n,1)\n",
    "\n",
    "    # call the machine interface to obtain observations\n",
    "    Y = np.array([accelerator.get(x1=x) for x in X]).reshape(n,1)\n",
    "\n",
    "    # quick scatter plot\n",
    "    plt.scatter(X,Y, marker='x')\n",
    "    plt.title('Observations')\n",
    "\n",
    "else:\n",
    "    # simulate the prior distribution by adding a point \n",
    "    # far away from the domain of interest.\n",
    "    # This is necessary because GPy does not allow an empty data set.\n",
    "    X, Y = np.array([[10e10]]), np.array([[0.]])\n",
    "\n",
    "# Define a kernel. We use RBF (or squared exponential to start with)\n",
    "rbf = GPy.kern.RBF(input_dim=1, lengthscale=1, variance=10)\n",
    "# Define the GP object\n",
    "gp = GPy.models.GPRegression(X=X, Y=Y, kernel=rbf, noise_var=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and variance predictions\n",
    "\n",
    "The posterior mean and variance can be obtain via `y,v = gp.predict_noiseless(X1)` for some evaluation point `X1` (we only change a single parameter here). If you are interested in including the noise in the predictive variance, you can use `y,v = gp.predict(X1)` instead. For Bayesian optimization, we are primarly intersted in the former, as our goal is to maximize the expected return value $f(x)$ of the target function.\n",
    "\n",
    "#### Things to try:\n",
    "* Change the evaluation point X1\n",
    "* use `.predict(X1)` instead of `predict_noiseless(X1)` to include the noise variance.\n",
    "* Increase the number of data points or use the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([[0.2]])  # need shape=(1,1)\n",
    "\n",
    "# For BO we are typically interested in the uncertainty in the posterior mean (f(x))\n",
    "# Since this distribution is Gaussian, it is specified by its mean and variance,\n",
    "# which we obtain form the following call:\n",
    "Y1, V1 = gp.predict_noiseless(X1)\n",
    "\n",
    "# to include the noise varianc, use \n",
    "# Y1, V1 = gp.predict(X1)  \n",
    "\n",
    "print(f\"Posterior at x = {float(X1)}:  y = {float(Y1):.2} Â± {float(np.sqrt(V1)):.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot mean and variance of our GP model, we have to first define a grid of evaluation points `X` and then call `Y,V = gp.predict_noiseless(X)` to obtain the preditive mean and variance. Then, we can compute predictive intervals via `Y - np.sqrt(V),  Y + np.sqrt(V)`. All of this is handled in the helper function `plot_1d_gp`, which was defined above. Have a look there how the plots are generated: Besides the plotting it uses nothing more than what was introduced in the previous cell.\n",
    "\n",
    "\n",
    "#### Things to try:\n",
    "* Change the evaluation set with `xlim`.\n",
    "* Change the number of data points (going back to the data-set definition above)\n",
    "* Go to the definition of `plot_1d_gp` above to see how we obtain the predictive intervals from the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_gp(gp, xlim=(-8,5), plot_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the GP\n",
    "\n",
    "We can obtain a posterior sample from our GP by calling `gp.posterior_samples_f(X_eval, size=1)` evaluated on points `X_eval`. Plotting functionality is provided via `plot_1d_gp_samples` as defined above.\n",
    "\n",
    "#### Things to try:\n",
    "\n",
    "* Re-run the cell to obtain different samples\n",
    "* Sample from the prior distribution\n",
    "* Add more data points\n",
    "* Go to the definition of `plot_1d_gp_samples` to see how we can obtain posterior samples from the gp\n",
    "* Generate the samples directly from the posterior mean and covariance (cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_gp_samples(gp, xlim=(-4,4), num_samples=5)\n",
    "plt.title('GP Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermezzo: Generating GP samples\n",
    "Recall that a $GP(m,K)$ is a distribution of functions f such that every finite marginal $f(x_1), ..., f(x_n)$ is a multivariate Gaussian with mean $[m(x_1), ..., m(x_n)]$ and covariance $[Cov(x_i, x_j)] = [K(x_i,x_j)]$.\n",
    "\n",
    "This can be used to generate the sample paths manually, and that is exactly what is behind `.posterior_samples_f(...)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define evaluation grid\n",
    "X_eval = np.linspace(-4,4,100).reshape(-1,1)\n",
    "\n",
    "# Step 2: Compute posterior mean and covariance matrix\n",
    "m, K = gp.predict_noiseless(X_eval, full_cov=True)\n",
    "\n",
    "# Step 3: Sample a multi-variate normal N(m,K)\n",
    "Y_sample = np.random.multivariate_normal(m.flatten(), K)\n",
    "\n",
    "# plotting\n",
    "plt.plot(X_eval.flatten(), Y_sample, color='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Hyperparameters\n",
    "\n",
    "Go back and generate a data set with `n=20` points. We will now explore what happens if we change the GP hyper-parameters.\n",
    "\n",
    "#### Things to try:\n",
    "* Change the lengthscale. Observe how you start overfitting with too small lengthscale, and underfitting with too large lengthscale.\n",
    "* Change the variance: This is the range of observation values we expect.\n",
    "* Change the noise variance: The larger this value, the more robust the regression is against pertubations in the observations.\n",
    "* Can you explain what happens if we set the noise variance to a very small value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kern.lengthscale = 1  # lengthscale, e.g. 0.1, 1., 10.\n",
    "gp.kern.variance = 10  # prior variance 1, 10, 100\n",
    "gp.likelihood.variance = 0.1  # noise variance e.g. 1, 0.1, 0.01, 0.0001, 0.00001.\n",
    "\n",
    "_, (axis1, axis2) = plt.subplots(1, 2, sharey=True, figsize=(10,3.5))\n",
    "\n",
    "# plot model uncertainty on axis1\n",
    "axis1.set_title('Model uncertainty')\n",
    "plot_1d_gp(gp, xlim=(-4,4), \n",
    "           plot_data=True,\n",
    "           plot_confidence=True,\n",
    "           axis=axis1)\n",
    "\n",
    "# plot samples on axis2\n",
    "axis2.set_title('Posterior Samples')\n",
    "plot_1d_gp_samples(gp, num_samples=5, xlim=(-4,4),\n",
    "                   plot_data=True,\n",
    "                   axis=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Hyperparameters\n",
    "\n",
    "GPy also offers a way of optimizing the hyper-parameters via the negative log-likelihood. Simply call `gp.optimize()`, or better `gp.optimize_restarts()`. The latter uses multiple restarts to escape local minima, as the likelihood is non-convex in general. Be careful however that for a good point-estiamte, you will need representative prior data of your optimization problem. Optimizing the paramters with data collected from the optimization often does not work very well.\n",
    "\n",
    "#### Things to try\n",
    "* Use a very small number (n=2) or a large number (n=100) of data points.\n",
    "* Do the parameters found agree with the 'good' settings found above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.optimize_restarts()  # optimizes parameters via log-likelihood\n",
    "\n",
    "display(gp)  # print resulting gp parameters\n",
    "\n",
    "\n",
    "# plotting, as before \n",
    "_, (axis1, axis2) = plt.subplots(1, 2, sharey=True, figsize=(10,3.5))\n",
    "\n",
    "# plot model uncertainty on axis1\n",
    "axis1.set_title('Model uncertainty')\n",
    "plot_1d_gp(gp, xlim=(-4,4), \n",
    "           plot_data=True,\n",
    "           plot_confidence=True,\n",
    "           axis=axis1)\n",
    "\n",
    "# plot samples on axis2\n",
    "axis2.set_title('Posterior Samples')\n",
    "plot_1d_gp_samples(gp, num_samples=5, xlim=(-4,4),\n",
    "                   plot_data=True,\n",
    "                   axis=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Kernel\n",
    "\n",
    "So far we have been using the RBF (or squared exponential) kernel. This is a common starting point for exploring the GP regression. However, depending on the function we want to model, choosing a different kernel can make a lot of sense. \n",
    "\n",
    "#### Things to try:\n",
    "* Uncomment the different kernels below and see how the regression and sample plots look like\n",
    "* You can also vary GP hyperparameter as before, \n",
    "* Uncomment the call to `.optimize_restarts()` to automatically adjust the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.Bias(input_dim=1)  # a constant kernel\n",
    "# kernel = GPy.kern.Linear(input_dim=1) \n",
    "\n",
    "# kernel = GPy.kern.Bias(input_dim=1) + GPy.kern.Linear(input_dim=1)\n",
    "# kernel = GPy.kern.Poly(input_dim=1, order=2)\n",
    "# kernel = GPy.kern.Matern32(input_dim=1, lengthscale=1., variance=10)\n",
    "# kernel = GPy.kern.Exponential(input_dim=1, lengthscale=1., variance=10)\n",
    "\n",
    "# kernel = GPy.kern.Matern52(input_dim=1, lengthscale=0.5, variance=10)\n",
    "# kernel = GPy.kern.RBF(input_dim=1, lengthscale=0.5, variance=10)\n",
    "\n",
    "# Combine different kernels\n",
    "# kernel = GPy.kern.Bias(input_dim=1) \\\n",
    "#             + GPy.kern.Matern32(input_dim=1, lengthscale=0.5) \\\n",
    "#             + GPy.kern.RBF(input_dim=1, lengthscale=0.5) \n",
    "\n",
    "gp = GPy.models.GPRegression(X=X, Y=Y, kernel=kernel, noise_var=0.1)\n",
    "\n",
    "# uncomment the next line if you want to optimize the hyperparamters\n",
    "# gp.optimize_restarts()  # optimizes parameters via log-likelihood\n",
    "\n",
    "display(gp)  # print resulting gp parameters\n",
    "\n",
    "fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(10,3.5))\n",
    "\n",
    "\n",
    "axis1.set_title(\"Model Uncertainty\")\n",
    "plot_1d_gp(gp, xlim=(-4,4), \n",
    "           plot_data=True,\n",
    "           plot_confidence=True,\n",
    "           axis=axis1, s=1)\n",
    "\n",
    "axis2.set_title(\"Posterior Samples\")\n",
    "plot_1d_gp_samples(gp, xlim=(-4,4),\n",
    "                   plot_data=True,\n",
    "                   axis=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Regression in Higher Dimensions\n",
    "\n",
    "GP regression works the same in higher dimensions. We start by defining 2d plotting helpers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_contour(gp, boundaries, axis=None, fig=None):\n",
    "    \"\"\"\n",
    "    Helper function for a 2d contour plot of a GP.\n",
    "    \"\"\"\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    if axis is None:\n",
    "        axis = fig.gca()\n",
    "    \n",
    "    # plot data\n",
    "    axis.scatter(gp.X[:,0], gp.X[:,1], marker='x', zorder=10)\n",
    "    \n",
    "    # create evaluation grid\n",
    "    x = np.linspace(*boundaries[0], 100)\n",
    "    y = np.linspace(*boundaries[1], 100)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    Xeval = np.vstack((xv.flatten(), yv.flatten())).T\n",
    "    \n",
    "    # query gp mean prediction\n",
    "    Ypred = gp.predict_noiseless(Xeval)[0].reshape(100,100)\n",
    "    \n",
    "    # show colorbar\n",
    "    colorbar = axis.contourf(xv,yv,Ypred, alpha=0.5)\n",
    "    fig.colorbar(colorbar)\n",
    "    \n",
    "    # set boundaries\n",
    "    axis.set_xlim(boundaries[0])\n",
    "    axis.set_ylim(boundaries[1])\n",
    "\n",
    "\n",
    "def plot_gp_3d(gp, boundaries, axis=None, fig=None):\n",
    "    \"\"\"\n",
    "    Helper function for a 3d plot of a GP. The axis, if passed as argument, needs to use\n",
    "    projection='3d'\n",
    "    \"\"\"\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    if axis is None:\n",
    "        axis = fig.gca(projection='3d')\n",
    "    \n",
    "    # plot data\n",
    "    axis.scatter(gp.X[:,0], gp.X[:,1], gp.Y[:,0], marker='x', zorder=10)\n",
    "    \n",
    "    # create evaluation grid\n",
    "    x = np.linspace(*boundaries[0], 100)\n",
    "    y = np.linspace(*boundaries[1], 100)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    Xeval = np.vstack((xv.flatten(), yv.flatten())).T\n",
    "    \n",
    "    # query gp mean prediction\n",
    "    Ypred = gp.predict_noiseless(Xeval)[0].reshape(100,100)\n",
    "    \n",
    "    # plot mesh\n",
    "    axis.plot_wireframe(xv,yv,Ypred, alpha=0.3)\n",
    "    \n",
    "    # set boundaries\n",
    "    axis.set_xlim(boundaries[0])\n",
    "    axis.set_ylim(boundaries[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a 2d data set. For simplicity, we evaluate our accelerator dummy at `n=20` uniformly random points. As before, you can play around with then number of data points, the kernel and optimizing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain boundaries\n",
    "boundaries = [(-1,1), (-2,2)]\n",
    "\n",
    "n = 20  # number of data points\n",
    "\n",
    "# get some random evaluation points\n",
    "X_2d = np.vstack((np.random.uniform(*boundaries[0], size=n),\n",
    "               np.random.uniform(*boundaries[1], size=n))).T\n",
    "\n",
    "# get data\n",
    "Y_2d = np.array([[accelerator.get(x=x)] for x in X_2d])\n",
    "\n",
    "# set up the gp model with a Matern52 kernel\n",
    "matern52 = GPy.kern.Matern52(lengthscale=0.5, input_dim=2, variance=2)\n",
    "gp_2d = GPy.models.GPRegression(X=X_2d, Y=Y_2d, kernel=matern52, noise_var=0.1)\n",
    "\n",
    "# optionally, optimize hyperparameters\n",
    "# gp_2d.optimize_restarts()\n",
    "\n",
    "display(gp_2d)\n",
    "\n",
    "# set up the plots\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3.5)\n",
    "axis1 = plt.subplot(121, projection='3d')\n",
    "axis2 = plt.subplot(122)\n",
    "\n",
    "# 3d plot of posterior mean\n",
    "plot_gp_3d(gp_2d, boundaries, axis=axis1)\n",
    "\n",
    "# contour plot of posterior mean\n",
    "plot_gp_contour(gp_2d, boundaries, axis=axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
