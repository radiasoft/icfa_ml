{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Bayesian Optimization\n",
    "\n",
    "In the second part of the tutorial, we are going to use the GPy library and Gaussian process regression to optimize a black-box function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple benchmark problem, we use a small mockup machine interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accelerator:\n",
    "    def __init__(self,sigma = 1.0):\n",
    "        self.x = np.array([0.1, 0.2])\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def get(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns the signal measurement.\n",
    "        \n",
    "        Examples:\n",
    "            get()  # returns a measurement with the current parameter settings\n",
    "            get(x=[0.1,0.4])  # sets the parameters x1=0.1, x2=0.4, then returns a measurement\n",
    "            get(x1=0.2)  # sets x1=0.2 and keeps the second parameter fixed\n",
    "            get(x1=0.1, x2=0.4)  # sets x1=0.1, x2=0.4\n",
    "        \n",
    "        Optional parameters:\n",
    "            x: array-like 2d\n",
    "            x1: scalar\n",
    "            x2: scalar\n",
    "        \"\"\"\n",
    "        if 'x' in kwargs:\n",
    "            self.x = np.array(kwargs['x'])\n",
    "        else:\n",
    "            for i in range(len(self.x)):\n",
    "                self.x[i] = kwargs.get(f'x{i+1}', self.x[i])\n",
    "        \n",
    "        xx, yy = self.x[0], self.x[1]\n",
    "        y = (4. - 2.1*xx**2 + (xx**4)/3.)*(xx**2) + xx*yy + (-4. + 4*(yy**2))*(yy**2)\n",
    "        return np.maximum(-y + 2.5, 0) + np.sqrt(self.sigma)*np.random.normal(0,1)\n",
    "\n",
    "\n",
    "def plot_1d_gp(gp, xlim, \n",
    "               plot_data=True, \n",
    "               plot_confidence=False, \n",
    "               s=2,\n",
    "               axis=None, color='C0'):\n",
    "    \"\"\"\n",
    "    Plots a 1d gp.\n",
    "    \"\"\"\n",
    "    \n",
    "    if axis is None:\n",
    "        axis = plt.gca()\n",
    "        \n",
    "    # define an evaluation set\n",
    "    X_eval = np.linspace(*xlim, 100).reshape(-1,1)  # GPy expects shape (num_points, input_dim)\n",
    "    \n",
    "    # compute posterior mean and variance\n",
    "    Y_pred, Y_var = gp.predict_noiseless(X_eval)\n",
    "    _, Y_noise_var = gp.predict(X_eval)\n",
    "    \n",
    "    # flatten the arrays for pyplot\n",
    "    X_eval = X_eval.reshape(-1)\n",
    "    Y_pred = Y_pred.reshape(-1)\n",
    "    Y_var = Y_var.reshape(-1)\n",
    "    Y_noise_var = Y_noise_var.reshape(-1)\n",
    "\n",
    "    # plot mean\n",
    "    axis.plot(X_eval, Y_pred, zorder=5, color=color, label = \"GP mean\")\n",
    "    \n",
    "    # plot data if it's not the 'fake' data point\n",
    "    if plot_data and not (gp.X.shape[0] == 1 and gp.X[0] > 0.9*10e10):\n",
    "        axis.scatter(gp.X, gp.Y, zorder=10, marker='x', color=color, label = \"data\")\n",
    "        \n",
    "    # plot the predictive uncertainty interval\n",
    "    if plot_confidence:\n",
    "        axis.fill_between(X_eval, Y_pred-s*np.sqrt(Y_var), Y_pred+s*np.sqrt(Y_var), alpha=0.4, color=color)\n",
    "\n",
    "    axis.set_xlim(xlim)  # set x-axis limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Grid optimizer - 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.1 ## std of noise level\n",
    "accelerator_no_noise = Accelerator(sigma = 0.0) # accelerator without noise\n",
    "accelerator_noise = Accelerator(sigma = noise_level) # accelerator with realistic noise\n",
    "\n",
    "n = 1000 # number of grid points (fine)\n",
    "X_orig = X = np.linspace(-2,2,n) # fine grid of the parameter domain\n",
    "\n",
    "no_grid_points = 10 # initial small grid \n",
    "grid = np.linspace(-2,2,no_grid_points) # initial small grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid with no noise fails due to efficiency even with noisless oracle: $y = f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_no_noise = np.array([[accelerator_no_noise.get(x1=x)] for x in X]) # Y data\n",
    "grid_values = np.array([[accelerator_no_noise.get(x1=x)] for x in grid]) # X data\n",
    "\n",
    "# plot the noiseless grid\n",
    "plt.plot(X,Y_no_noise,\"r-\",alpha = 0.7)\n",
    "plt.plot(grid,grid_values,'bo')\n",
    "\n",
    "print (\"Maximum achieved: \", np.max(grid_values), \"Optimality gap:\", np.max(Y_no_noise) - np.max(grid_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having no noise is unrealistic, hence we add observational noise: $y = f(x) + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = np.array([[accelerator_noise.get(x1=x)] for x in grid])\n",
    "plt.plot(X,Y_no_noise,alpha = 0.7, lw = 3, color= 'red', label = \"true function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y_no_noise,alpha = 0.7, lw = 3, color = \"red\", label = \"true function\")\n",
    "plt.plot(grid,grid_values,'bx', label = \"grid - 10 points\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Initial data and confidence bounds - 1d\n",
    "\n",
    "In this example, we will see how probabilistic model (GP) allows us to restrict the search space using high probabilistic confidence estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(-3,5)\n",
    "plt.xlim(-2,2)\n",
    "plt.plot(X,Y_no_noise,alpha = 0.7, lw = 3, color = \"red\", label = \"true function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small initial grid grid \n",
    "no_small_grid_points = 5\n",
    "grid_small = np.linspace(-2,2,no_small_grid_points)\n",
    "grid_small_value = np.array([[accelerator_noise.get(x1=x)] for x in grid_small])\n",
    "grid_small = grid_small.reshape(-1,1)\n",
    "\n",
    "# Define a kernel\n",
    "rbf = GPy.kern.RBF(input_dim=1, lengthscale=0.5, variance=5)\n",
    "# Define the GP object\n",
    "gp = GPy.models.GPRegression(X=grid_small,\n",
    "                             Y=grid_small_value,\n",
    "                             kernel=rbf, noise_var=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(-3,5)\n",
    "plt.xlim(-2,2)\n",
    "plot_1d_gp(gp, xlim=(-2,2),\n",
    "            plot_data = True,\n",
    "            plot_confidence = True)\n",
    "plt.plot(X_orig,Y_no_noise,alpha = 0.7, lw = 3, color = \"red\", label = \"true function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(-3,5)\n",
    "plt.xlim(-2,2)\n",
    "plot_1d_gp(gp, xlim=(-2,2),\n",
    "            plot_data = True,\n",
    "            plot_confidence = True)\n",
    "# get mean and variance from the gp model\n",
    "Y_mean, Y_var = gp.predict_noiseless(X.reshape(-1,1))\n",
    "# plot the lower confidence bound\n",
    "plt.plot(X_orig,X_orig*0+np.max(Y_mean-2*np.sqrt(Y_var)),'--',color = \"orange\")\n",
    "plt.plot(X_orig,Y_no_noise,alpha = 0.7, lw = 3, color = \"red\", label = \"true function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = np.max(Y_mean-2*np.sqrt(Y_var)) # maximum of lower confidence estimate (95%)\n",
    "ucb = Y_mean+2*np.sqrt(Y_var) # upper confidence estimate (95%)\n",
    "discarted = X_orig[(ucb<limit)[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(-3,5)\n",
    "plt.xlim(-2,2)\n",
    "plot_1d_gp(gp, xlim=(-2,2),\n",
    "            plot_data = True,\n",
    "            plot_confidence = True)\n",
    "plt.plot(X_orig,X_orig*0+np.max(Y_mean-2*np.sqrt(Y_var)),'--',color = \"orange\")\n",
    "plt.plot(X_orig,Y_no_noise,alpha = 0.7, lw = 3, color = \"red\", label = \"true function\")\n",
    "plt.ylim(-3,5)\n",
    "plt.xlim(-2,2)\n",
    "i = 0\n",
    "for x in X_orig:\n",
    "    if (ucb<limit)[i,0]:\n",
    "        plt.fill_between([x], -3,5, alpha=0.4, color='orange')\n",
    "    i=i+1\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) UCB algorithm - 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB_step(X,Y,beta,kernel,discretization):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns an optimizer of a UCB acquisiton function given a GP model with kernel and data (X,Y). \n",
    "    Discretization determines the optimization space for the acquisiton function    \n",
    "    \"\"\"  \n",
    "    \n",
    "    # Define the GP object\n",
    "    gp = GPy.models.GPRegression(X=X,\n",
    "                                 Y=Y,\n",
    "                             kernel=kernel, noise_var=noise_level)\n",
    "    \n",
    "    # predict mean and variance\n",
    "    Y_mean, Y_var = gp.predict_noiseless(discretization)\n",
    "\n",
    "    # construct acquisition function\n",
    "    acquisition_function = Y_mean + beta * np.sqrt(Y_var)\n",
    "    \n",
    "    # maximize it to get candidate point\n",
    "    candidate_point = discretization[np.argmax(acquisition_function)]\n",
    " \n",
    "    return (candidate_point,gp)\n",
    "\n",
    "def plot_acquisition(gp,discretization):\n",
    "    # predict mean and variance\n",
    "    Y_mean, Y_var = gp.predict_noiseless(discretization.reshape(-1,1))\n",
    "\n",
    "    # construct acquisition function\n",
    "    acquisition_function = Y_mean + beta * np.sqrt(Y_var)\n",
    "    \n",
    "    # maximize it to get candidate point\n",
    "    candidate_point = discretization[np.argmax(acquisition_function)]\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(discretization, acquisition_function,'g',label = \"UCB acquisition\")\n",
    "    plt.plot(candidate_point, np.max(acquisition_function), marker = 's', color = 'g') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reuse previous small grid as initial dataset for our Bayesian Optimization Algorithm\n",
    "X = grid_small\n",
    "Y = grid_small_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines design choices as kernel and the exploration/exploitation parameter. You can change and play with the settings to see the effect and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a kernel\n",
    "kernel = GPy.kern.RBF(input_dim=1, lengthscale=0.5, variance=5)\n",
    "\n",
    "# Define discretization of the domain\n",
    "discretization = np.linspace(-2,2,1000).reshape(-1,1)\n",
    "\n",
    "# We define the exploration/exploitation parameter according to a common heuristic\n",
    "beta = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code implements one step of GP-UCB algorithm. When rerunning the cells next step of the BO algorithm is performed and the current posterior of GP is plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a kernel\n",
    "kernel = GPy.kern.RBF(input_dim=1, lengthscale=0.5, variance=5)\n",
    "\n",
    "# Define discretization of the domain\n",
    "discretization = np.linspace(-2,2,1000).reshape(-1,1)\n",
    "\n",
    "# We define the exploration/exploitation parameter according to a common heuristic\n",
    "beta = 2.\n",
    "\n",
    "# acquisiton of the next data point\n",
    "x_new, gp = UCB_step(X,Y,beta,kernel,discretization) # make a UCB step\n",
    "\n",
    "# evaluation of the new data point\n",
    "y_new = accelerator_noise.get(x1 = x_new)\n",
    "\n",
    "# appending the new data point\n",
    "X = np.append(X,x_new.reshape(-1,1), axis = 0)\n",
    "Y = np.append(Y,y_new.reshape(-1,1), axis =0)\n",
    "\n",
    "# plot everything\n",
    "plot_1d_gp(gp, xlim=(-2,2),\n",
    "            plot_data = True,\n",
    "            plot_confidence = True)\n",
    "plot_acquisition(gp,discretization)\n",
    "plt.plot(X_orig,Y_no_noise,label = \"true function\", color = \"r\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_UCB_optimize(budget,kernel,beta,X_init,Y_init, discretization, output = None):\n",
    "    \"\"\"\n",
    "    Implements GP-UCB algorithm\n",
    "    \n",
    "    budget (int) : number of steps to be taken\n",
    "    kernel (GPY object): kernel type\n",
    "    beta (float): exploration/exploitation parameter\n",
    "    X_init(numpy 2D) : initial data\n",
    "    Y_init(numpy 2D) : initial data\n",
    "    discretization (numpy 2D): discretization array for aquisition function optimization\n",
    "    \n",
    "    \"\"\"\n",
    "    X = X_init\n",
    "    Y = Y_init\n",
    "    \n",
    "    d = X_init.shape[1]\n",
    "    \n",
    "    for t in range(budget):\n",
    "\n",
    "        # acquisiton of the next data point\n",
    "        x_new, gp = UCB_step(X,Y,beta,kernel,discretization) # make a UCB step\n",
    "    \n",
    "        # evaluation of the new data point\n",
    "        if d > 1:\n",
    "            y_new = accelerator_noise.get(x1 = x_new[0], x2 = x_new[1])\n",
    "        else:\n",
    "            y_new = accelerator_noise.get(x1 = x_new)\n",
    "\n",
    "        # appending the new data point\n",
    "        X = np.append(X,x_new.reshape(-1,d), axis = 0)\n",
    "        Y = np.append(Y,y_new.reshape(-1,1), axis = 0)\n",
    "        \n",
    "    Y_mean, Y_var = gp.predict_noiseless(discretization)\n",
    "    best_point = discretization[np.argmax(Y_mean)]\n",
    "    if output == \"full\":\n",
    "        return best_point, gp, X, Y\n",
    "    else:\n",
    "        return best_point, gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following experiment we will use the same number of point in order to optimize the function as the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization \n",
    "optimal_point, gp = GP_UCB_optimize(7,kernel,3.0,grid_small,grid_small_value,discretization)\n",
    "\n",
    "# determining the location and value of optimal point\n",
    "index = np.abs(X_orig-optimal_point).argmin()\n",
    "optimal_value = Y_no_noise[index]\n",
    "print (\"Optimal point: \", optimal_point)\n",
    "print (\"Maximum achieved: \",  optimal_value, \"Optimality gap: \", np.max(Y_no_noise) -  optimal_value)\n",
    "\n",
    "# final plot\n",
    "\n",
    "# plot everything\n",
    "plot_1d_gp(gp, xlim=(-2,2),\n",
    "            plot_data = True,\n",
    "            plot_confidence = True)\n",
    "plot_acquisition(gp,discretization)\n",
    "plt.plot(X_orig,Y_no_noise,label = \"true function\", color = \"r\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) 2d example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a kernel\n",
    "kernel_2d = GPy.kern.RBF(input_dim=2, lengthscale=0.5, variance=5) # note the change to 2 dimensions\n",
    "\n",
    "# Define discretization of the domain (2D)\n",
    "x = np.linspace(-2,2,50)\n",
    "y = np.linspace(-1,1,50) # note the second parameter varries over different domain\n",
    "discretization_2d = np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))])\n",
    "\n",
    "# We define the exploration/exploitation parameter according to a common heuristic\n",
    "beta = 2.\n",
    "\n",
    "# initial data\n",
    "N = 3 # initial number of points\n",
    "\n",
    "X_2d = np.random.uniform(low = -1, high = 1, size = (N,2))\n",
    "Y_2d = np.array([[accelerator_noise.get(x=x)] for x in X_2d])\n",
    "\n",
    "# running the method\n",
    "T = 10\n",
    "optimal_point, gp, X, Y = GP_UCB_optimize(T,kernel_2d,2.0,X_2d,Y_2d,discretization_2d, output = \"full\")\n",
    "\n",
    "print (\"Optimal point: \", optimal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# predict mean and var:\n",
    "Y_mean, _  = gp.predict_noiseless(discretization_2d)\n",
    "Y_mean_max = gp.predict_noiseless(np.array([optimal_point]))\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.axes(projection='3d')\n",
    "xx = discretization_2d[:, 0]\n",
    "yy = discretization_2d[:, 1]\n",
    "grid_x, grid_y = np.mgrid[min(xx):max(xx):100j, min(yy):max(yy):100j]\n",
    "grid_z_mu = griddata((xx, yy), Y_mean[:, 0], (grid_x, grid_y), method='linear')\n",
    "ax.scatter(X[:, 0], X[:, 1], Y[:,0], c='r', s=100, marker=\"o\", depthshade=False)\n",
    "ax.scatter(optimal_point[0],optimal_point[1], Y_mean_max[0], c='b', s=100, marker=\"o\", depthshade=False)\n",
    "ax.plot_surface(grid_x, grid_y, grid_z_mu, color='r', alpha=0.4)\n",
    "plt.title('Posterior mean prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
